{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9430c241-06c1-4175-b264-393c82bbf74a",
   "metadata": {},
   "source": [
    "## Scarping Github Top repositories for features topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d859be-556b-4f29-84b6-3a1fdb2c5b3e",
   "metadata": {},
   "source": [
    "**Web scraping** is the process of automatically extracting data from websites. It enables users to collect structured information from web pages for analysis, automation, and integration into various applications. This technique is widely used in data science, business intelligence, and competitive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe0ad0-2fb5-41a3-8613-91ed40cb03af",
   "metadata": {},
   "source": [
    "**GitHub** is a version control and collaboration platform where developers manage and share code. It categorizes projects based on topics, making it easier to discover repositories related to specific technologies and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9bd59-81ee-4b1f-8b31-cdd68c52d12a",
   "metadata": {},
   "source": [
    "**Project Objective-**\n",
    "In this project, we aim to create CSV files containing repository and user details for the top repositories under featured topics on GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb713a-2938-4690-87d7-1d6f3af563a1",
   "metadata": {},
   "source": [
    "The tools used in this project include:\n",
    "- Python: The core programming language for data extraction and processing.\n",
    "- Requests: To make API calls and fetch data from GitHub.\n",
    "- BeautifulSoup: For web scraping GitHub topics and repository details.\n",
    "- OS: To manage file operations and directories.\n",
    "- Pandas: For data manipulation and saving extracted details into CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9310e5-cbea-4e82-91f6-fc002af5ee88",
   "metadata": {},
   "source": [
    "**Here are the steps we'll follow:**\n",
    "\n",
    "- We're going to scrape 'https://github.com/topics'\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic, we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Username, repo name,Stars,Repo URL\n",
    "mrdoob,three.js,69700,https://github.com/mrdoob/three/js\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732b67c-40ec-4cc0-a75e-19d47779d09c",
   "metadata": {},
   "source": [
    "#### Installing the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4adae411-08ac-4624-b29d-6034fb591222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce14c4-e408-4802-9cdc-5f27a25638f3",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b560131a-df56-4c94-bdc6-3731910f17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c1a55-4c63-4ae4-8fae-9bde64e625f6",
   "metadata": {},
   "source": [
    "#### Scrape the list of topics from GitHub\n",
    "Steps to follow:\n",
    "- Use requests to download the page\n",
    "- Use beautiful soup to parse and extract the information\n",
    "- Convert the data lists to pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2d97a-bbc6-48d7-be3b-72a629bfcf2b",
   "metadata": {},
   "source": [
    "##### Create a function to access and download the page and return an object of type bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75003d3e-2c01-46ff-b63e-3d1767591e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Base Url for accessing pages further and is declared globally. (Can also be declared wherever required instead)\n",
    "base_url = 'https://github.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b992dd8-2508-4901-b795-8376cd670464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # Get the page from the given topic URL\n",
    "    response = requests.get(topic_url)\n",
    "    # Check the status of the page is successful else print the error\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse the web page to beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02702507-3fe8-41dd-8a57-b24b869c7ccb",
   "metadata": {},
   "source": [
    "##### Create the function to parse and extract data using beautiful soup which return the dataframe containing the topics data in the format - topic_title, topic_description and topic_url\n",
    "- First fetch the list of tags using ***find*** and ***find_all*** methods of bs4\n",
    "- Fetch the data from the tags for creating a list of titles, descriptions and urls for topics and store in the pandas dataframes\n",
    "- *(Optional) Save the topics list as a CSV file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29732bba-5a31-4951-a8b5-1c0aae991d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_from_github():\n",
    "    # Get the page located at 'topics_url'\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    doc = get_topic_page(topics_url)\n",
    "    \n",
    "    # Preparing the dataframe of topics page by extracting the titles, descriptions and urls for each topic -\n",
    "    topics_dict = {\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description': get_topic_descs(doc),\n",
    "        'url': get_topic_urls(doc)\n",
    "    }\n",
    "    # Convert the data into dataframe and return the df\n",
    "    topics_df = pd.DataFrame(topics_dict)\n",
    "\n",
    "    # Create a CSV file for dataframe to store the topics details\n",
    "    # topics_df.to_csv('github_topics.csv', index = False)\n",
    "    \n",
    "    return topics_df\n",
    "\n",
    "# Helper function which takes bs4 object containing topic details and extract the title lists\n",
    "def get_topic_titles(doc):\n",
    "    topic_selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': topic_selection_class})\n",
    "    topic_titles = []\n",
    "    for title in topic_title_tags:\n",
    "        topic_titles.append(title.text.strip())\n",
    "    return topic_titles\n",
    "\n",
    "# Helper function which takes bs4 object containing topic details and extract the descriptions lists\n",
    "def get_topic_descs(doc):\n",
    "    desc_selection_class = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selection_class})\n",
    "    topic_descs = []\n",
    "    for desc in topic_desc_tags:\n",
    "        topic_descs.append(desc.text.strip())\n",
    "    return topic_descs\n",
    "\n",
    "# Helper function which takes bs4 object containing topic details and extract the urls lists\n",
    "def get_topic_urls(doc):\n",
    "    links_selection_class = 'no-underline flex-1 d-flex flex-column'\n",
    "    topic_link_tags = doc.find_all('a', {'class': links_selection_class})\n",
    "    #Aim - https://github.com/topics/3d\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for url in topic_link_tags:\n",
    "        topic_urls.append(base_url + url['href'].strip())\n",
    "    return topic_urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af069e2c-f98a-427d-9ab5-2fb30accdbb5",
   "metadata": {},
   "source": [
    "##### Getting information for each topic page using the topic url and saving to CSV file\n",
    "- Open and download the topic URL page\n",
    "- Create a directory to store all your files\n",
    "- Parse and extract the details like - username, repository name, stars and repository URL\n",
    "- Create the topic-wise CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8fac2-0989-4f4a-afac-0b74a3f711e2",
   "metadata": {},
   "source": [
    "##### These are the helper function to help in fetching details from topics page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc34d568-992d-4c1d-be08-d53f4b3220cb",
   "metadata": {},
   "source": [
    "##### This is the function which take tags as parameters and extract the data from them to return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d596c8a5-7f8e-4713-bba3-409ab10e842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to fetch details - username, repo name, stars, repo url for repository\n",
    "def get_repo_details(header_tags, star_tags):\n",
    "    # Returns all the information about the repositories\n",
    "    a_tags = header_tags.find_all('a')\n",
    "    username = a_tags[0].text\n",
    "    repo_name = a_tags[1].text\n",
    "    # We need to convert our stars strinng to appropiate integer\n",
    "    stars = parse_stars(star_tags.text)\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    \n",
    "    return username, repo_name, stars, repo_url\n",
    "\n",
    "\n",
    "# Function to convert the star string value to integer\n",
    "def parse_stars(star):\n",
    "    star = star.strip()\n",
    "    if star[-1].lower() == 'k':\n",
    "        return int(float(star[:-1]) * 1000)\n",
    "    return int(star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72539525-9b12-42f2-8f2c-989a5130a7a3",
   "metadata": {},
   "source": [
    "##### A function which take the bs4 object of each topic page as input and creates a dataframe for each topic containing the top repositories details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6065a20-b7aa-4c88-ab03-ddd52cc3df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to fetch the details\n",
    "def get_repos_for_topics(topic_doc):\n",
    "    # Extract the repositories tags from the page\n",
    "    repo_tags = topic_doc.find_all('h3', {'class' : 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    star_tags = topic_doc.find_all('span', {'class' : 'Counter js-social-count'})\n",
    "    \n",
    "    # Create a dict to store the details\n",
    "    topics_repo_dict = {\n",
    "        'username' : [],\n",
    "        'repository_name' : [],\n",
    "        'stars' : [],\n",
    "        'repository_url' : []\n",
    "        }\n",
    "    \n",
    "    # Traverse through each repository to fetch the details\n",
    "    for i in range(len(repo_tags)):\n",
    "        username, repo_name, stars, repo_url = get_repo_details(repo_tags[i], star_tags[i])\n",
    "        #print(i, username, repo_name, stars, repo_url)\n",
    "        topics_repo_dict['username'].append(username)\n",
    "        topics_repo_dict['repository_name'].append(repo_name)\n",
    "        topics_repo_dict['stars'].append(stars)\n",
    "        topics_repo_dict['repository_url'].append(repo_url)\n",
    "\n",
    "    return pd.DataFrame(topics_repo_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c7bf8-0625-43d8-992a-f4f364741798",
   "metadata": {},
   "source": [
    "Below methods are created to parse each topic pages using the above implemented helper functions and create a directory to store generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d811aa46-8841-4bb1-a2d8-56654951f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create the directory in the current path\n",
    "def create_dir():\n",
    "    dirname = \"Github_topics_files\"\n",
    "    try:\n",
    "        os.mkdir(dirname)\n",
    "    except FileExistsError:\n",
    "        print(\"Directory already exists.\")\n",
    "    except OSError as err:\n",
    "        print(f\"Error creating directory: {err}\")\n",
    "    print('Directory {} created.'.format(dirname))\n",
    "    return dirname\n",
    "\n",
    "# Function to open the topic page and save the details to csv files in the created folder\n",
    "def parse_topic_page(topic_url, topic_name, dir):\n",
    "    print('Processing {}...'.format(topic_name))\n",
    "    # Fetch the topic page with url = topic_url\n",
    "    topic_doc = get_topic_page(topic_url)\n",
    "    # Use the page to extract repositories\n",
    "    topic_repos_df = get_repos_for_topics(topic_doc)\n",
    "\n",
    "    # Create a CSV file of this df\n",
    "    fname = topic_name + '.csv'\n",
    "    fpath = '{}/{}'.format(dir, fname)\n",
    "    print('Generting the file {} for topic {}...'.format(fname, topic_name))\n",
    "    topic_repos_df.to_csv(fpath, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfce864-0f9d-4a25-9d72-d10658137c0f",
   "metadata": {},
   "source": [
    "##### This is the main function (driver) where the program starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15be655-93ec-4326-9438-e725641b8a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_github_main():\n",
    "    # Get the list of topics from github Topics page\n",
    "    print('Getting the list of top featured topics from Github...')\n",
    "    topics_df = scrape_topics_from_github()\n",
    "\n",
    "    # Create a directory\n",
    "    dirname = create_dir()\n",
    "    \n",
    "    # Fetch each topics page and create the csv file\n",
    "    for index, row in topics_df.iterrows():\n",
    "        parse_topic_page(row['url'], row['title'], dirname)\n",
    "\n",
    "    print('All files are generated in the directory {}'.format(dirname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56cc56-f182-4b4c-aeac-f765c50ccc62",
   "metadata": {},
   "source": [
    "Execute below code cell to scrape the top repositories for all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9544ecc-05f2-4132-ad4a-38144d755a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the list of top featured topics from Github...\n",
      "Directory Github_topics_files created.\n",
      "Processing 3D...\n",
      "Generting the file 3D.csv for topic 3D...\n",
      "Processing Ajax...\n",
      "Generting the file Ajax.csv for topic Ajax...\n",
      "Processing Algorithm...\n",
      "Generting the file Algorithm.csv for topic Algorithm...\n",
      "Processing Amp...\n",
      "Generting the file Amp.csv for topic Amp...\n",
      "Processing Android...\n",
      "Generting the file Android.csv for topic Android...\n",
      "Processing Angular...\n",
      "Generting the file Angular.csv for topic Angular...\n",
      "Processing Ansible...\n",
      "Generting the file Ansible.csv for topic Ansible...\n",
      "Processing API...\n",
      "Generting the file API.csv for topic API...\n",
      "Processing Arduino...\n",
      "Generting the file Arduino.csv for topic Arduino...\n",
      "Processing ASP.NET...\n",
      "Generting the file ASP.NET.csv for topic ASP.NET...\n",
      "Processing Awesome Lists...\n",
      "Generting the file Awesome Lists.csv for topic Awesome Lists...\n",
      "Processing Amazon Web Services...\n",
      "Generting the file Amazon Web Services.csv for topic Amazon Web Services...\n",
      "Processing Azure...\n",
      "Generting the file Azure.csv for topic Azure...\n",
      "Processing Babel...\n",
      "Generting the file Babel.csv for topic Babel...\n",
      "Processing Bash...\n",
      "Generting the file Bash.csv for topic Bash...\n",
      "Processing Bitcoin...\n",
      "Generting the file Bitcoin.csv for topic Bitcoin...\n",
      "Processing Bootstrap...\n",
      "Generting the file Bootstrap.csv for topic Bootstrap...\n",
      "Processing Bot...\n",
      "Generting the file Bot.csv for topic Bot...\n",
      "Processing C...\n",
      "Generting the file C.csv for topic C...\n",
      "Processing Chrome...\n",
      "Generting the file Chrome.csv for topic Chrome...\n",
      "Processing Chrome extension...\n",
      "Generting the file Chrome extension.csv for topic Chrome extension...\n",
      "Processing Command-line interface...\n",
      "Generting the file Command-line interface.csv for topic Command-line interface...\n",
      "Processing Clojure...\n",
      "Generting the file Clojure.csv for topic Clojure...\n",
      "Processing Code quality...\n",
      "Generting the file Code quality.csv for topic Code quality...\n",
      "Processing Code review...\n",
      "Generting the file Code review.csv for topic Code review...\n",
      "Processing Compiler...\n",
      "Generting the file Compiler.csv for topic Compiler...\n",
      "Processing Continuous integration...\n",
      "Generting the file Continuous integration.csv for topic Continuous integration...\n",
      "Processing C++...\n",
      "Generting the file C++.csv for topic C++...\n",
      "Processing Cryptocurrency...\n",
      "Generting the file Cryptocurrency.csv for topic Cryptocurrency...\n",
      "Processing Crystal...\n",
      "Generting the file Crystal.csv for topic Crystal...\n",
      "All files are generated in the directory Github_topics_files\n"
     ]
    }
   ],
   "source": [
    "scrape_github_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a5ee4-2b72-4fa7-8f94-0a8614b5ffe2",
   "metadata": {},
   "source": [
    "#### Validation steps\n",
    "Once our code has executed, following validations are required to ensure if the files are properly generated -\n",
    "- First we need to make sure, all the files are generated with proper names and within proper directory.\n",
    "- Next, we need to validate the content of the files, they must be comma-seperated files with 4 columns - user name, repository name, stars, repository URL. Make sure to check few pages with the github site, if proper names are displayed and in proper order (top-bottom).\n",
    "- Additonally, we can use pandas to open the CSV files and validate the data and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ba651-1626-4940-bf5a-5afaff678872",
   "metadata": {},
   "source": [
    "## **Conclusion**  \r\n",
    "With this project, we successfully scraped and stored details of the top repositories for featured topics on GitHub. Using **Python, Requests, BeautifulSoup, OS, and Pandas**, we automated the extraction of key repository and user details, making it easier to analyze trending projects.  \r\n",
    "\r",
    "#\n",
    "## **Useful References**  \r\n",
    "During development, the following resources were particularly helpful:  \r\n",
    "- [GitHub Topics](https://github.com/topics) â€“ Repository categories and featured topics  \r\n",
    "- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#) â€“ Web scraping guide  \r\n",
    "- [Python Pathlib](https://docs.python.org/3/library/pathlib.html) â€“ File system operations  \r\n",
    "- [Python Input/Output](https://docs.python.org/3/tutorial/inputoutput.html) â€“ Data handling  \r\n",
    "- [YouTube Tutorial](https://www.youtube.com/live/RKsLLG-bzEY?si=gHjxeLaRf35veaIF) â€“ Related learning resou#rce  \r\n",
    "\r\n",
    "## **Future Enhancements**  \r\n",
    "This project can be extended with additional features, such as:  \r\n",
    "- **Data Analysis**: Using Python libraries to analyze trends in GitHub topics.  \r\n",
    "- **Multi-Page Scraping**: Extracting data from multiple pages for deeper insights.  \r\n",
    "- **User Interaction**: Allowing users to input topics and customize the data extraction process.  \r\n",
    "\r\n",
    "This is just the beginningâ€”thereâ€™s much more to expl:) and enhance! ðŸš€ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa574631-ebd3-4aa9-989a-d38eb8a583af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
